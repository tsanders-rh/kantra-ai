# kantra-ai Configuration File Example
# Place this file as .kantra-ai.yaml in your project directory or home directory
# CLI flags will override these settings

# AI Provider Configuration
provider:
  name: claude       # claude, openai, groq, ollama, together, anyscale, perplexity, openrouter, lmstudio
  model: ""          # Optional: claude-sonnet-4-20250514, gpt-4, llama-3.1-70b-versatile, codellama, etc.
  base-url: ""       # Optional: Custom base URL for OpenAI-compatible APIs (auto-set for presets)

# Input/Output Paths
paths:
  analysis: ""  # Path to Konveyor output.yaml (e.g., ./analysis/output.yaml)
  input: ""     # Path to source code directory (e.g., ./src)

# Cost and Effort Limits
limits:
  max-cost: 0.0   # Maximum spending in USD (0 = no limit)
  max-effort: 0   # Only fix violations with effort <= this value (0 = no limit)

# Filtering Options
filters:
  categories: []      # Filter by category, e.g., ["mandatory", "optional"]
  violation-ids: []   # Filter by specific IDs, e.g., ["javax-to-jakarta-001"]

# Git Integration
git:
  commit-strategy: ""  # per-violation, per-incident, or at-end (empty = no commits)
  create-pr: false     # Automatically create GitHub pull requests (requires commit-strategy and GITHUB_TOKEN)
  branch-prefix: ""    # Custom branch name prefix (default: kantra-ai/remediation-TIMESTAMP)
                       # Note: Actual branch names may include violation IDs or indices depending on strategy

# Build/Test Verification
verification:
  enabled: false      # Enable build/test verification after fixes
  type: test          # "build" or "test"
  strategy: at-end    # per-fix, per-violation, or at-end
  command: ""         # Custom verification command (empty = auto-detect)
  fail-fast: true     # Stop on first verification failure

# Confidence Threshold Filtering
# Controls whether to apply AI-generated fixes based on confidence scores and migration complexity
confidence:
  enabled: false               # Enable confidence threshold filtering (default: false for backward compatibility)
  min-confidence: 0.0          # Global minimum confidence (0.0-1.0, 0.0 = use complexity-based thresholds)
  on-low-confidence: skip      # Action for low-confidence fixes: skip, warn-and-apply, manual-review-file
  complexity-thresholds:       # Override default thresholds per complexity level (optional)
    # Default thresholds (uncomment to override):
    # trivial: 0.70   # 95%+ AI success - mechanical find/replace
    # low: 0.75       # 80%+ AI success - straightforward API equivalents
    # medium: 0.80    # 60%+ AI success - requires context understanding
    # high: 0.90      # 30-50% AI success - architectural changes
    # expert: 0.95    # <30% AI success - domain expertise required

# General Settings
dry-run: false  # Preview changes without applying them

# Example configurations for common scenarios:

# Minimal config - fix all violations with Claude:
# provider:
#   name: claude
# paths:
#   analysis: ./analysis/output.yaml
#   input: ./src

# Conservative config - only mandatory violations, with cost limit:
# provider:
#   name: claude
# paths:
#   analysis: ./analysis/output.yaml
#   input: ./src
# limits:
#   max-cost: 5.00
# filters:
#   categories: ["mandatory"]

# Full automation - fixes with git commits, PR creation, and testing:
# provider:
#   name: claude
# paths:
#   analysis: ./analysis/output.yaml
#   input: ./src
# git:
#   commit-strategy: per-violation  # Creates one commit per violation type
#   create-pr: true                  # Creates one PR per violation (requires GITHUB_TOKEN)
# verification:
#   enabled: true
#   type: test
#   strategy: at-end

# PR creation with custom branch name:
# provider:
#   name: claude
# paths:
#   analysis: ./analysis/output.yaml
#   input: ./src
# git:
#   commit-strategy: at-end
#   create-pr: true
#   branch-prefix: feature/konveyor-migration  # Single PR with custom branch name

# Using Groq (fast, affordable inference):
# provider:
#   name: groq
#   model: llama-3.1-70b-versatile
# paths:
#   analysis: ./analysis/output.yaml
#   input: ./src

# Using Ollama (local, free, private):
# provider:
#   name: ollama
#   model: codellama  # or llama3, deepseek-coder, etc.
# paths:
#   analysis: ./analysis/output.yaml
#   input: ./src

# Using Together AI (open source models):
# provider:
#   name: together
#   model: meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo
# paths:
#   analysis: ./analysis/output.yaml
#   input: ./src

# Using OpenRouter (100+ models):
# provider:
#   name: openrouter
#   model: meta-llama/llama-3.1-70b-instruct
# paths:
#   analysis: ./analysis/output.yaml
#   input: ./src

# Using custom OpenAI-compatible API:
# provider:
#   name: openai
#   base-url: https://your-api.com/v1
#   model: your-model
# paths:
#   analysis: ./analysis/output.yaml
#   input: ./src

# Conservative mode - skip low-confidence fixes for safety:
# provider:
#   name: claude
# paths:
#   analysis: ./analysis/output.yaml
#   input: ./src
# confidence:
#   enabled: true
#   on-low-confidence: skip  # Only apply high-confidence fixes

# Aggressive mode - apply all fixes but warn about low confidence:
# provider:
#   name: claude
# paths:
#   analysis: ./analysis/output.yaml
#   input: ./src
# confidence:
#   enabled: true
#   on-low-confidence: warn-and-apply  # Apply everything, but warn

# Custom confidence thresholds - stricter for complex migrations:
# provider:
#   name: claude
# paths:
#   analysis: ./analysis/output.yaml
#   input: ./src
# confidence:
#   enabled: true
#   complexity-thresholds:
#     high: 0.95    # Require very high confidence for complex changes
#     expert: 0.98  # Require near-perfect confidence for expert-level changes
